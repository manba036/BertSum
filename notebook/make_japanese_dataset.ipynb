{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Japanese detaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import mojimoji\n",
    "import pandas as pd\n",
    "from fastprogress import progress_bar\n",
    "from pyknp import Juman\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_FILES = '../raw_data/*.json'\n",
    "TOKENIZED_FILE = '../merged_stories_tokenized/{}.json'\n",
    "MAP_VALID_FILENAME = '../urls/mapping_valid.txt'\n",
    "MAP_TEST_FILENAME = '../urls/mapping_test.txt'\n",
    "MAP_TRAIN_FILENAME = '../urls/mapping_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jm = Juman()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Sentence Splitting and Tokenization\n",
    "\n",
    "{id}.jsonなファイルに下記データを格納\n",
    "```json\n",
    "{ 'sentences': [\n",
    "    { 'tokens': [\n",
    "        { 'word': \"@highlight\" }\n",
    "    ]},\n",
    "    { 'tokens': [\n",
    "        { 'word': \"タイトル\" },\n",
    "        { 'word': \"...\" },\n",
    "        ...\n",
    "    ]},\n",
    "    { 'tokens': [\n",
    "        { 'word': \"要約\" },\n",
    "        { 'word': \"...\" },\n",
    "        ...\n",
    "    ]},\n",
    "    ...\n",
    "]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(RAW_FILES)\n",
    "for file in progress_bar(files):\n",
    "    with codecs.open(file, 'r', 'utf-8') as fin:\n",
    "        docs = json.load(fin)\n",
    "        for doc in docs:\n",
    "            if doc['要約'] == '' or doc['タイトル'] == '':\n",
    "                continue\n",
    "\n",
    "            srcs = doc['要約'].split('\\n')\n",
    "            if len(srcs) < 3:\n",
    "                continue\n",
    "\n",
    "            data = { 'sentences': [] }\n",
    "\n",
    "            tgt_zen = mojimoji.han_to_zen(doc['タイトル'])\n",
    "            res = jm.analysis(tgt_zen)\n",
    "            tokens = [ i.midasi for i in res.mrph_list() ]\n",
    "            data['sentences'].append({\n",
    "                'tokens': [ { 'word': '@highlight' } ]\n",
    "            })\n",
    "            data['sentences'].append({\n",
    "                'tokens': [ { 'word': token } for token in tokens ]\n",
    "            })\n",
    "\n",
    "            for src in srcs:\n",
    "                src_zen = mojimoji.han_to_zen(src)\n",
    "                res = jm.analysis(src_zen)\n",
    "                tokens = [ i.midasi for i in res.mrph_list() ]\n",
    "                if len(tokens) == 0:\n",
    "                    continue\n",
    "                data['sentences'].append({\n",
    "                    'tokens': [ { 'word': token } for token in tokens ]\n",
    "                })\n",
    "\n",
    "            id  = doc['URL'].split(\"/\")[-2]\n",
    "            with codecs.open(TOKENIZED_FILE.format(id), 'w', 'utf-8') as fout:\n",
    "                json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls ../merged_stories_tokenized | grep 'json' > ../merged_stories_tokenized/list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../merged_stories_tokenized/list.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_and_valid = train_test_split(df, test_size=0.1, random_state=40)\n",
    "test, valid = train_test_split(test_and_valid, test_size=0.5, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.to_csv(MAP_VALID_FILENAME, header=None, index=False)\n",
    "test.to_csv(MAP_TEST_FILENAME, header=None, index=False)\n",
    "train.to_csv(MAP_TRAIN_FILENAME, header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Format to Simpler Json Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /work/BertSum-japanese/src\n",
    "python3 preprocess.py \\\n",
    " -mode format_to_lines \\\n",
    " -raw_path ../merged_stories_tokenized \\\n",
    " -save_path ../json_data/japanese \\\n",
    " -map_path ../urls \\\n",
    " -n_cpus 2 \\\n",
    " -log_file ../logs/preprocess_step4.log\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Format to PyTorch Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /work/BertSum-japanese/src\n",
    "python3 preprocess.py \\\n",
    " -mode format_to_bert \\\n",
    " -raw_path ../json_data \\\n",
    " -save_path ../bert_data \\\n",
    " -oracle_mode greedy \\\n",
    " -n_cpus 2 \\\n",
    " -log_file ../logs/preprocess_step5.log\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# for GPU\n",
    "cd /work/BertSum-japanese/src\n",
    "python3 train.py \\\n",
    " -mode train \\\n",
    " -encoder transformer \\\n",
    " -bert_config_path ../models/Japanese_L-12_H-768_A-12_E-30_BPE/bert_config.json \\\n",
    " -dropout 0.1 \\\n",
    " -bert_data_path ../bert_data/japanese \\\n",
    " -model_path ../models/bert_transformer_japanese_gpu \\\n",
    " -lr 2e-3 \\\n",
    " -visible_gpus 0 \\\n",
    " -gpu_ranks 0 \\\n",
    " -world_size 1 \\\n",
    " -report_every 50 \\\n",
    " -save_checkpoint_steps 5000 \\\n",
    " -batch_size 3000 \\\n",
    " -decay_method noam \\\n",
    " -train_steps 50000 \\\n",
    " -accum_count 2 \\\n",
    " -log_file ../logs/bert_transformer_japanese_gpu \\\n",
    " -use_interval true \\\n",
    " -warmup_steps 10000 \\\n",
    " -ff_size 2048 \\\n",
    " -inter_layers 2 \\\n",
    " -heads 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# for CPU\n",
    "cd /work/BertSum-japanese/src\n",
    "python3 train.py \\\n",
    " -mode train \\\n",
    " -encoder transformer \\\n",
    " -bert_config_path ../models/Japanese_L-12_H-768_A-12_E-30_BPE/bert_config.json \\\n",
    " -dropout 0.1 \\\n",
    " -bert_data_path ../bert_data/japanese \\\n",
    " -model_path ../models/bert_transformer_japanese_cpu \\\n",
    " -lr 2e-3 \\\n",
    " -report_every 50 \\\n",
    " -save_checkpoint_steps 5000 \\\n",
    " -batch_size 3000 \\\n",
    " -decay_method noam \\\n",
    " -train_steps 50000 \\\n",
    " -accum_count 2 \\\n",
    " -log_file ../logs/bert_transformer_japanese_cpu \\\n",
    " -use_interval true \\\n",
    " -warmup_steps 10000 \\\n",
    " -ff_size 2048 \\\n",
    " -inter_layers 2 \\\n",
    " -heads 8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
